{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering\n",
    "\n",
    "There are three datasets needed for this analysis. \n",
    "Firstly, the brand list is a catalog of all brands offered by Sephora. \n",
    "Next, we use the brand list and search for a detailed product list within each branch product_id like 'P07102'. \n",
    "Lastly, the product_id is then used in gathering the review data.\n",
    "\n",
    "Since there is already existing data parsed by Raghad Alharbi, we will skip Step 1 and Step 2.\n",
    "- Sephora result data from Kaggle: https://www.kaggle.com/raghadalharbi/all-products-available-on-sephora-website\n",
    "\n",
    "Tutorial for data gathering https://github.com/Shirleyiscool/Scraping-Sephora\n",
    "- Step1 Brand list: we will use requests to the website https://www.sephora.com/brands-list and parse a list of brands\n",
    "- Step2 Product list: for each brand list, we use requests to the individual website \"https://www.sephora.com\"+brand.a.attrs['href']+\"/all?pageSize=300\" and parse a list of products.\n",
    "- Step3 Review list: for each product, we use an API call to bazaarvoice and get all reviews under each product.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional filters\n",
    "Due to large amount of products and reviews, we limited our scope according to the goal: look for products that brings happiness to people during pandemic. Thus below filters are applied.\n",
    "\n",
    "- Keep only product with review count > 50\n",
    "- Keep only product with review star >= 4\n",
    "- Keep only the most recent 3100 revies. (Certain product reviews are too large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all packages are imported on top since we have to parse data in multiple days. \n",
    "# Thus the individual section has separate imports\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: Brand list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getctime Response of \"brandlist\" Website from Sephora\n",
    "band_lst_link = \"https://www.sephora.com/brands-list\"\n",
    "response = requests.get(band_lst_link)\n",
    "\n",
    "# Use BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping brand links and save them into a list\n",
    "brand_link_lst = []\n",
    "main_box = soup.find_all(attrs={\"data-comp\": \"BrandsList StyledComponent BaseComponent \"})[0]\n",
    "for brand in main_box.find_all('li'):\n",
    "    brand_link_lst.append(\"https://www.sephora.com\" +\n",
    "                          brand.a.attrs['href']+\"/all?pageSize=300\")\n",
    "\n",
    "# Write brand links into a file:\n",
    "with open('data/brand_link.txt', 'w') as f:\n",
    "    for item in brand_link_lst:\n",
    "        f.write(f\"{item}\\n\")\n",
    "\n",
    "# Indicate scraping completion\n",
    "print(f'Got All Brand Links! There are {len(brand_link_lst)} brands in total.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Product list\n",
    "\n",
    "In this step, we use the brand list to look for all products inside the brand. \n",
    "\n",
    "- In Step2.1, we gather all products from the brand page.\n",
    "- In Step2.1, we gather information regarding each product from the product page.\n",
    "\n",
    "### Step2.1: Get all products from brand list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_brand = 1\n",
    "test_product = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.ctime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scape_product(link, proxy=None):\n",
    "    \"\"\"\n",
    "    A function to scape all the product links from a given brand link.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(link, proxies={\n",
    "                                \"http\": proxy, \"https\": proxy}, timeout=15)\n",
    "    except:\n",
    "        print(f'\\r Unsuccessfully get data for {link.split(\"/\")[4]}', end=\"\")\n",
    "        return None\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    product_link_lst = []\n",
    "    try:\n",
    "        product_box = soup.find_all(attrs={\"data-comp\": \"ProductGrid \"})[0]\n",
    "    # There might be no products for that brand\n",
    "    except IndexError:\n",
    "        return []\n",
    "    for product in product_box.find_all('a',\n",
    "                                        attrs={\"data-comp\": \"ProductItem \"}):\n",
    "        # use function split to remove text like \"grid p12345\"\n",
    "        product_link_lst.append(\n",
    "            \"https://www.sephora.com\" + product.attrs['href'].split()[0])\n",
    "    return product_link_lst\n",
    "\n",
    "\n",
    "# Read brand links file\n",
    "product_link_dic = {'brand': [], 'product_links': []}\n",
    "# num_lines = sum(1 for line in open(\"data/brand_link.txt\", \"r\"))\n",
    "num_lines = test_brand\n",
    "\n",
    "# Scape all the product links from all the brands links.\n",
    "# This will take some time!\n",
    "ct = 1\n",
    "\n",
    "# Get proxies from http://www.freeproxylists.net/zh/?c=US&pr=HTTPS&u=80&s=ts\n",
    "px = ['143.198.222.22:8080', '143.198.206.183:8080', '157.230.208.88:8080']\n",
    "px_idx = 0\n",
    "\n",
    "for brand_link in open(\"data/brand_link.txt\", \"r\"):\n",
    "    if ct<=test_product:\n",
    "        brand_name = brand_link.split('/')[4]\n",
    "        product_link_list = scape_product(brand_link[:-1], proxy=px[px_idx])\n",
    "\n",
    "        # If one proxy does not work, use another\n",
    "        while product_link_list is None:\n",
    "            px_idx += 1\n",
    "            if px_idx == 3:\n",
    "                px_idx = 0\n",
    "            product_link_list = scape_product(brand_link[:-1], proxy=px[px_idx])\n",
    "\n",
    "        print(f'\\r === {ct} / {num_lines} ===  {brand_name} === {px[px_idx]}',\n",
    "              end=\"\")\n",
    "        product_link_dic['brand'] += [brand_name] * len(product_link_list)\n",
    "        product_link_dic['product_links'] += product_link_list\n",
    "        ct += 1\n",
    "\n",
    "# Write the result into csv file\n",
    "product_link_df = pd.DataFrame(product_link_dic)\n",
    "product_link_df.to_csv('data/product_links.csv', index=False)\n",
    "\n",
    "# Indicate scraping completion\n",
    "print(f'Got All product Links! There are {len(product_link_df)} products in '\n",
    "      f'total.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.2 Get details on product website\n",
    "For example, likes, review total, price, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(product_link, px_list=None):\n",
    "    \"\"\"Get product information\"\"\"\n",
    "    data_dic = {'pd_id': [], 'size_and_item': [], 'category': [],\n",
    "                'price': [], 'love_count': [], 'reviews_count': []}\n",
    "    px_idx = 0\n",
    "    proxy = None if px_list is None else px_list[px_idx]\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(product_link, proxies={\n",
    "                \"http\": proxy, \"https\": proxy}, timeout=15)\n",
    "        except:\n",
    "            if px_idx == len(px_list) - 1:\n",
    "                px_idx = 0\n",
    "            else:\n",
    "                px_idx += 1\n",
    "            proxy = px_list[px_idx]\n",
    "            continue\n",
    "\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        data_dic['pd_id'] = re.findall(R'P[0-9]{3,6}', product_link)[0]\n",
    "\n",
    "        # Get Category\n",
    "        try:\n",
    "            cat_box = soup.find_all(attrs={'data-comp': 'ProductBreadCrumbs BreadCrumbs '})[0]\n",
    "            cat_list = [cat.string for cat in cat_box.find_all('a')]\n",
    "            category = ', '.join(cat_list)\n",
    "        except:\n",
    "            category = None\n",
    "\n",
    "\n",
    "        category\n",
    "\n",
    "        # Size and Content\n",
    "        try:\n",
    "            size_and_item = soup.find(\n",
    "                attrs={\"data-at\": \"sku_size_label\"}).get_text()\n",
    "        except:\n",
    "            size_and_item = None\n",
    "\n",
    "        size_and_item\n",
    "\n",
    "        # Get Price\n",
    "        try:\n",
    "            price = soup.find_all(attrs={'data-comp': 'Price '})[\n",
    "                0].get_text()\n",
    "        except:\n",
    "            price = None\n",
    "\n",
    "        price\n",
    "\n",
    "\n",
    "        # Get love counts\n",
    "        try:\n",
    "            love_count = soup.find('span', attrs={\n",
    "                \"class\": \"css-jk94q9\"}).get_text()\n",
    "        except:\n",
    "            love_count = None\n",
    "\n",
    "        love_count\n",
    "\n",
    "\n",
    "        # review nums\n",
    "        try:\n",
    "        #     link_json = soup.find(attrs={\"id\": \"linkJSON\"})\n",
    "        #     json_str = str(link_json)\n",
    "        #     reviews = re.findall(R'\\\"reviews\\\":(.*?)\\,', json_str)\n",
    "            reviews = soup.find('span', attrs = {\n",
    "        #         'class':\"css-nv7myq eanm77i0\",\n",
    "#                 'class': \"css-1vj6vps eanm77i0\"\n",
    "        #         'data-comp' : 'StyledComponent BaseComponent ',\n",
    "        #         'id': 'ratings-reviews-container'\n",
    "                'data-at': 'number_of_reviews'\n",
    "            }).get_text()\n",
    "            reviews_count = reviews\n",
    "        except:\n",
    "            reviews_count = None\n",
    "\n",
    "\n",
    "        data_dic['category'] = category\n",
    "        data_dic['size_and_item'] = size_and_item\n",
    "        data_dic['love_count'] = love_count\n",
    "        data_dic['reviews_count'] = reviews_count\n",
    "        data_dic['price'] = price\n",
    "        break\n",
    "    return data_dic\n",
    "\n",
    "\n",
    "px_list_ = [\n",
    "            '167.99.218.191:8080',\n",
    "            '144.217.254.175:3128', \n",
    "            '165.225.77.47:9443',\n",
    "            '54.37.137.211:3128', \n",
    "            '165.22.91.197:8080',\n",
    "            '165.225.77.47:8800', \n",
    "            '165.225.77.47:9400',\n",
    "            '165.225.77.47:80', \n",
    "            '165.225.77.47:443',\n",
    "            '143.198.222.22:8080', \n",
    "            '143.198.206.183:8080', \n",
    "            '157.230.208.88:8080',\n",
    "            ]\n",
    "\n",
    "pd_links_df = pd.read_csv('data/product_links.csv')\n",
    "product_links = pd_links_df['product_links']\n",
    "\n",
    "result = []\n",
    "for i, link in enumerate(product_links[:]):\n",
    "    result.append(get_data(link, px_list_))\n",
    "    pd_df = pd.DataFrame(result)\n",
    "    pd_df.to_csv('data/pd_info.csv', index=False)\n",
    "    print(f'{i + 1:04d} / {len(product_links)} || {link}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.ctime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Duration: ', {end - start})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3: Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load product link data frame.\n",
    "# Here we used full data from Kaggle.\n",
    "# In the project, we also limited the products using the filters mentioned above.\n",
    "pd_links_df = pd.read_parquet('./data/pd_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load product link data frame.\n",
    "# Here we used full data from Kaggle.\n",
    "# In the project, we also limited the products using the filters mentioned above.\n",
    "pd_links_df = pd.read_parquet('./data/sephora_website_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_links_df['product_links'] = pd_links_df['URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_products_already_downloaded(pd_links_df):\n",
    "    'Product reviews are too large (300 products for 600-800MB), thus we download in multiple batches.'\n",
    "    with open('data/product_keys.pkl', 'rb') as file:\n",
    "        lst = pickle.load(file)\n",
    "\n",
    "    pids_cleaned = lst\n",
    "    pids = list(result)\n",
    "    pd_links_df['key'] = pd_links_df['URL'].str.findall('P[0-9]{4,7}').apply(lambda x: x[0])\n",
    "    pd_links_df = pd_links_df[~pd_links_df['key'].isin(pids_cleaned)].copy()\n",
    "    \n",
    "    return pd_links_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if this is the first time\n",
    "# pd_links_df = remove_products_already_downloaded(pd_links_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_links_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column of product id\n",
    "pd_links_df['pd_id'] = [re.findall('P[0-9]{4,7}', link)[0] for link\n",
    "                        in pd_links_df['product_links']]\n",
    "\n",
    "\n",
    "def scrape_reviews(p_id, proxy=None):\n",
    "    url = 'https://api.bazaarvoice.com/data/reviews.json'\n",
    "    params = {\n",
    "        'Filter': f'ProductId:{p_id}',\n",
    "        'Sort': 'SubmissionTime:desc',\n",
    "        'Limit': 100,\n",
    "        'Offset': 0,\n",
    "        'Include': 'Products,Comments',\n",
    "        'Stats': 'Reviews',\n",
    "        'passkey': 'caQ0pQXZTqFVYA1yYnnJ9emgUiW59DXA85Kxry8Ma02HE',\n",
    "        'apiversion': 5.4,\n",
    "        'Locale': 'en_US',\n",
    "    }\n",
    "\n",
    "    \n",
    "    reviews = []\n",
    "    loop = 0\n",
    "\n",
    "    while loop<=30:\n",
    "        params['Offset'] = len(reviews)\n",
    "\n",
    "        # Make the same request that Javascript makes\n",
    "        try:\n",
    "            r = requests.get(url, params=params, proxies={\n",
    "                \"http\": proxy, \"https\": proxy}, timeout=15)\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except:\n",
    "            print(f'{proxy} Cannot connect!')\n",
    "            return None, None\n",
    "        if loop == 0:\n",
    "            try:\n",
    "                product = r.json()['Includes']['Products']\n",
    "            except KeyError:\n",
    "                product = []\n",
    "\n",
    "        # break if we have an error or have all the reviews\n",
    "        if (r.status_code != 200) or (\n",
    "                len(reviews) >= r.json()['TotalResults']):\n",
    "            break\n",
    "\n",
    "        # add the list of results to current results\n",
    "        reviews.extend(r.json()['Results'])\n",
    "\n",
    "        # Give a pause, so we don't get blocked\n",
    "        time.sleep(0.2)\n",
    "        loop += 1\n",
    "\n",
    "    # Show how many reviews we scraped\n",
    "    print(f'{p_id}: {len(reviews)} reviews')\n",
    "    time.sleep(0.5)\n",
    "    return product, reviews\n",
    "\n",
    "\n",
    "# Scrape Product and Review Data\n",
    "# result already imported from file\n",
    "# result = {}\n",
    "\n",
    "proxies = [\n",
    "        '20.194.17.90:3128',\n",
    "        '69.167.174.17:80',\n",
    "        '129.226.52.93:443',\n",
    "        '164.90.222.95:80',\n",
    "        '143.55.38.198:8080',\n",
    "        '130.61.236.104:80',\n",
    "        '34.126.79.176:80',\n",
    "        '132.145.18.53:80',\n",
    "        '68.183.221.156:37486',\n",
    "        '143.198.196.205:80',\n",
    "        '140.227.63.136:58888',\n",
    "        '167.71.230.124:8080',\n",
    "        '148.66.131.212:80',\n",
    "        '173.249.38.220:8118',\n",
    "        '85.84.14.9:80',\n",
    "        '129.21.105.164:8080',\n",
    "        '190.9.55.12:8080',\n",
    "        '209.127.191.180:9279',\n",
    "        '208.74.51.100:80',\n",
    "        '159.65.174.145:3128',\n",
    "        '190.9.55.12:8080',\n",
    "        '45.95.96.187:8746',\n",
    "        '45.95.96.237:8796',\n",
    "        '45.94.47.66:8110',\n",
    "        '45.94.47.108:8152',\n",
    "        '45.95.99.226:7786',\n",
    "        '183.88.226.50:8080',\n",
    "        '52.151.15.4:80',\n",
    "        '51.81.82.175:80',\n",
    "        '129.21.158.30:8080',\n",
    "        '185.198.190.237:12444',\n",
    "        '149.125.70.236',\n",
    "        '167.99.118.98',\n",
    "        '92.204.129.161:80',\n",
    "        '52.168.34.113:80',\n",
    "        '74.205.128.201:80',\n",
    "        '209.97.150.167',\n",
    "        '191.96.42.80:8080',\n",
    "        '198.199.86.11:3128',\n",
    "        '198.199.86.11:3128',  \n",
    "]\n",
    "px_id = 0\n",
    "loop = 0\n",
    "\n",
    "for pid in pd_links_df['pd_id']:\n",
    "    loop_ = loop % 1000\n",
    "    if (loop_ < 900) and (loop_ >= 1):\n",
    "        product, reviews = None, None\n",
    "        while True:\n",
    "            if px_id == len(proxies):\n",
    "                px_id = 0\n",
    "\n",
    "            product_data, reviews_data = scrape_reviews(pid,\n",
    "                                                        proxy=proxies[px_id])\n",
    "            if product_data is not None:\n",
    "                break\n",
    "            px_id += 1\n",
    "\n",
    "    # Use my own server to connect\n",
    "    else:\n",
    "        product_data, reviews_data = scrape_reviews(pid)\n",
    "    loop += 1\n",
    "\n",
    "    print(f'{proxies[px_id]} || {loop:04d}/{len(pd_links_df)}')\n",
    "    result[pid] = [product_data, reviews_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/scraper_result.json\", \"w\") as file:\n",
    "    json.dump(result, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/product_keys.pkl\", \"wb\") as file:\n",
    "    pickle.dump(list(result.values()), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a backup file\n",
    "!cp ./data/scraper_result.json ./data/scraper_result.bak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
